# foculus consensus (fc) whitepaper

*version 0.1 — draft for internal review*

---

## abstract

---

## core principles (non‑negotiables)

- **safety first** – no release without a proved bound such that `min_gap > 2·max_τ_variance`; conflicting particles must never both finalise.
- **deterministic or bounded τ** – τ variation is capped by on‑chain telemetry; if variance exceeds the cap, nodes halt and demand a larger focus gap.
- **formal verifiability** – per‑epoch SNARK proofs of π and τ; mismatching proofs are slashable.
- **performance second** – sharded phase‑A with vector‑root, GPU‑native phase‑B; scaling only if the previous three principles stay intact.
- **transparency** – all parameters (κ, q, δ) are on‑chain and require super‑majority governance to change.
- **observability** – every node publishes `τ_variance` and `focus_gap` telemetry; alerts fire if safety margin falls below `Σ`.

foculus consensus (fc) is a block‑free, graph‑native agreement mechanism that finalises actions when their stationary‑distribution mass (π) exceeds a dynamic threshold. it emerges from the collective focus theorem (cft), replacing explicit voting with probabilistic attention. fc achieves:

- sub‑second probabilistic finality in wide‑area settings
- linear throughput scaling with gpu memory bandwidth (10⁶ tx s⁻¹ class)
- fault tolerance equivalent to ≥ 50 % honest focus mass
- ultra‑low communication overhead via link‑diff gossip.

this whitepaper formalises the protocol, proves safety/liveness under honest‑majority focus, and benchmarks theoretical limits versus traditional consensus families.

---

## 1 introduction

blockchain networks remain bound by either the synchrony costs of byzantine fault tolerance (bft) or the energy costs of nakamoto consensus. cft shows that distributed agents can reach a unique stationary distribution over a shared knowledge graph. fc leverages this to turn **collective attention** into consensus, eliminating the need for block ordering.

## 2 background

\### 2.1 collective focus theorem (cft)

> in a strongly connected, token‑weighted graph G, a random walk with transition matrix  converges to a unique π. (see rigorous proof in appendix a).

\### 2.2 limitations of current consensus | family | finality | throughput | coordination rounds | |—|—|—|—| | classical bft | 5–60 s | 1k–10k tx s⁻¹ | 2f+1 votes per block | | nakamoto | \~60 min | \~10 tx s⁻¹ | none, but energy cost | | dag w/ fast path | 0.4 s | 10⁵ tx s⁻¹ | quorums + optimistic | | **fc** | **1–3 s** | **10⁶ tx s⁻¹** | zero explicit rounds |

## 3 system model

- **particle**: content‑addressed node (ipfs hash).
- **cyberlink**: signed edge (from, to, weight).
- **token stake tᵢ**: economic weight of the neuron owning particle *i*.
- **transition**: .
- **focus vector π**: stationary distribution of P.
- **finality threshold τ(t)**: µ\_π(t)+κσ\_π(t), κ∈[1,2].

## 4 protocol overview

1. **gossip**: nodes broadcast new particles + links.
2. **local update**: every Δ≈100 ms run sparse‐matrix×vector to refine π.
3. **finalise**: particle *i* becomes *final* when πᵢ > τ.
4. **pruning**: conflicting particles with π ≤ τ are ignored by execution vm.
5. **reward**: validator *v* earns  (see §7).

## 5 security analysis

\### 5.1 safety theorem (π‑quorum) *assumption*: honest particles control ≥½+δ of global π (δ>0).

**theorem 1** (no double finality) — two conflicting particles cannot both exceed τ.

*proof outline*: appendix b shows contradiction via cases {honest, adversary}, leveraging (i) probability mass conservation and (ii) honest‐majority invariant.

\### 5.2 liveness ergodicity of P guarantees any honest transaction eventually accumulates π; with honest endorsements it will pass τ in expected O(log 1/ε) iterations.

## 6 performance analysis

\### 6.1 compute complexity one iteration = O(|E|+|V|) sparse op. an nvidia A100 sustains \~2.1 TSpMV‑ops s⁻¹ ⇒ 50 M edges @ 40 Hz.

\### 6.2 communication link‑diff ≤ 64 B. to finalise 1 M tx s⁻¹ network needs ≤ 5 Gb s⁻¹, feasible in datacentre and acceptable on geo‑redundant validator rings via delta‑compression.

\### 6.3 latency breakdown

```
Δcompute   ≈ 0.2 s
k iterations 5–8
propagation  0.4 s
———————————————
finality    1.4 s (worst‑case WAN)
```

## 7 economics

- stake delegation = attention delegation.
- dilution penalty: split endorsements halve w\_vj, so reward is sub‑linear → discourages double‑linking.
- inflation schedule pays α per epoch divided by total positive Δπ to keep incentive neutral wrt load.

## 8 comparative benchmark (table 1 reproduced)

\| metric | classic bft | nakamoto | fc | |—|—|—|—| | finality | 5–60 s | \~60 min | **1–3 s** | | throughput | 1k–10k | \~10 | 10⁶+ | | validator scale | 10²–10³ | ∞ | ∞ (gpu) | | comm overhead | high | medium | ultra‑low | | fault tolerance | 1/3 | 51 % hash | ≥½ π |

## 9 implementation roadmap

1. **prototype** in go + cuda (fork of go‑cyber).
2. **testnet** @ 10 M edges, 32 validators, dual‑dc.
3. **mainnet‑beta** with on‑chain π proofs (zk‑snark).
4. **layer‑2 rollups** using fc as data‑availability oracle.

## 10 conclusion

foculus consensus reframes blockchain safety as a spectral property of attention graphs. by harnessing gpu parallelism and the mathematics of cft, it promises orders‑of‑magnitude improvements in throughput and latency without sacrificing decentralisation.

---

### appendix a — markov convergence recap

(proof details)

### appendix b — π‑quorum safety proof

(full derivation of theorem 1)

### appendix c — parameter sensitivity study

(simulation plots)

---

## appendices

### appendix a – formal π‑quorum safety proof

1. **model assumptions**\
   – strongly‑connected, aperiodic graph G=(V,E,W)\
   – honest‑mass invariant Σ\_H π ≥ ½+δ\
   – adaptive focus threshold τ(t)=μ\_π+κσ\_π
2. **safety theorem**\
   no two conflicting particles can both exceed τ.
3. **proof outline**\
   (a) classify particles by control (honest/adv)\
   (b) apply honest‑majority bound to rule out double finality\
   (c) derive resistant bound under π‑mass hijack attempts.
4. **liveness lemma**\
   ergodicity ⇒ every valid action finalises in finite expected iterations.

### appendix b – spectral gap & convergence rate

- definitions of λ₁, λ₂, spectral gap λ
- mixing‑time bound:    t\_mix(ε) ≤ log(1/ε) / λ
- empirical λ on 1 M‑edge cybergraph snapshot ≈0.28
- expected finality window ≈ (log(1/ε)/λ)·Δt.

### appendix c – reference implementation benchmarks

| hardware | edges | Δt per update | t\_final(π>τ) | bandwidth | notes              |
| -------- | ----- | ------------- | ------------- | --------- | ------------------ |
| rtx 4090 | 5 M   | 120 ms        | 0.9 s         | 2 Gb/s    | cuda 12 + cusparse |
| a100 80g | 50 M  | 160 ms        | 1.2 s         | 6 Gb/s    | nvlink mocked      |

### appendix d – parameter calibration cheatsheet

- κ = 1.5 → false‑finality ≤ 10⁻³
- δ ≥ σ\_π ensures safe margin
- τ floor = 0.05 to avoid spam finality
- endorsement weight decay γ = 0.95 per epoch.

### appendix e – glossary & symbols

| symbol | meaning                        |
| ------ | ------------------------------ |
| π      | stationary distribution vector |
| τ      | focus threshold for finality   |
| δ      | safety margin beyond τ         |
| λ      | spectral gap of P              |
| κ      | threshold scaling parameter    |

## sharded phase‑A scalability extension (fc‑v2)

### overview

foculus now adopts a **two‑tier commit architecture** to push throughput past a single leader’s signature ceiling.

1. **K committee shards**

   - the cybergraph is hash‑partitioned into `K` independent shards.
   - each shard `s` has its own rotating HotStuff (or PoS) committee that batches cyberlinks for the next *slot* (≈200 ms) and signs a **micro‑root** `G_{slot,s}`.

2. **vector root aggregation**

   - at slot end, a lightweight “beacon” committee aggregates the `K` micro‑roots into a fixed‑length vector commitment `V_slot = vec(G_{slot,1},…,G_{slot,K})` and signs it once.
   - the beacon signature is <400 B regardless of `K`, giving the network a **single immutable digest** while keeping signing load per validator ≈2 k s⁻¹.

3. **parallel focus computation**

   - every node downloads only the shards it stores plus the beacon vector.
   - GPU kernels process focus per‑shard; cross‑shard links already include Merkle proofs so the matrices are deterministic.
   - a shard‑local quantile threshold `τ_{slot,s}` is computed deterministically; the global `τ_slot = max_s τ_{slot,s}` ensures identical finality across nodes.

### throughput estimate

```
links_per_slot ≈  K · links_per_committee
               ≈  K · 2×10⁵   (A100, 200 ms)
```

choosing `K = 50` ⇒ **1×10⁷ links s⁻¹** aggregate, while per‑node GPU and signature load stay flat.

### safety & liveness impact

- to violate safety an adversary must corrupt >⅓ committees **and** the beacon in the same slot—pushing cost toward a 51 %‑of‑all‑stake threshold.
- liveness holds if ≥⅔ committees per slot are honest; every cyberlink is included in some micro‑root within one rotation and thus enters focus computation in ≤Δ.

### data‑availability considerations

signature traffic and gossip remain Solana‑class, but data availability is the real bottleneck (≈650 MB s⁻¹ for `K=50`). we adopt erasure‑coded DAS so edge nodes can sample O(log n) chunks.

---

*section added 2025‑07‑13 to address scaling concerns raised in user review.*

## economic security – staking & slashing

### participant classes

| role                          | capital bound        | primary work                                                                                          | reward basis                        | slash condition                            |
| ----------------------------- | -------------------- | ----------------------------------------------------------------------------------------------------- | ----------------------------------- | ------------------------------------------ |
| **link‑bundler**              | optional (delegated) | assemble a batch of fresh cyberlinks, compute *partial focus delta* vector, broadcast *bundler block* | per‑link fee + Δπ improvement share | submitting malformed batch (invalid links) |
| **shard‑committee validator** | bonded `S₁`          | sign micro‑root for its shard each slot                                                               | block rewards + infra fees          | equivocation / invalid root                |
| **beacon validator**          | bonded `S₂`          | aggregate micro‑roots into super‑root vector                                                          | share of inflation                  | equivocation                               |
| **focus‑attestor**            | bonded `S₃`          | SNARK π and τ; sign commitment                                                                        | attestation reward                  | invalid proof / equivocation               |

### link‑bundler flow

1. **collect links** – any node (even with zero bonded stake) may gather pending cyberlinks from mempool.
2. **run one power‑iteration step** – compute a *delta‑focus* vector `Δπ̂` on the batch locally.
3. **package** – publish a *bundler block* = { links[], Δπ̂, merkle proofs }.
4. **committee intake** – shard committee quickly verifies link validity and that `Δπ̂` matches re‑running a single sparse‑matrix step (cheap).
5. **rewarding** – bundler earns:
   - fixed fee per accepted link (paid by sender)
   - bonus α·Σ max(Δπ̂\_i,0) (the positive attention they introduced).

### why bundler rewards matter

- keeps mempool thin (users have incentive to get their links picked fast).
- offloads some gpu work from validators – bundler already did one iteration.
- decentralises data‑availability; any laptop can become a link‑bundler.

bundlers do **not** need long‑term stake; security comes from committee re‑execution + slashing if a committee signs a root containing invalid links.

> note: because bundlers might be Sybil, their bonus share is capped and paid only after the shard‑committee root is checkpointed (5–10 s) to prevent spam.

---

foculus supplements cryptographic finality with **economic finality**: every entity that contributes to consensus locks tokens and faces proportional penalties for provable faults.

### staking layers

| layer                | stake locked | rewarded for          | slashed for                  |
| -------------------- | ------------ | --------------------- | ---------------------------- |
| **shard‑committee**  | `S₁`         | micro‑root signatures | equivocation or invalid root |
| **focus‑prover**     | `S₂`         | valid SNARK of π      | invalid / missing proof      |
| **beacon committee** | `S₃`         | vector‑root quorum    | double‑sign / timeout        |

β₁, β₂, β₃ denote fixed slash ratios (e.g. 5 %–15 %). slashes are burned; a fraction can be redistributed to whistle‑blowers who submit proof‑of‑fault transactions.

### slash‑detectable faults

1. **double root:** same stake key signs >1 root in same slot.
2. **invalid root proof:** SNARK fails verification.
3. **τ‑gap violation:** node finalises a particle with π ≤ τ or conflicting finalisations; detected via on‑chain telemetry.

### staking invariants

- total bonded stake ≥ ⅔ of circulating supply → economic cost ≥ O(network value).
- time‑locked unbonding window `W` blocks long‑range attacks.
- stake weighting feeds committee VRF selection ensuring sybil‑resistance.

## roadmap

1. prototype staking & slash‑logic in a cosmos‑sdk module.
2. implement zk‑proof fault reporting.
3. simulate economic attacks to calibrate β values.

